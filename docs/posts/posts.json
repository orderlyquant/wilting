[
  {
    "path": "posts/2021-02-10-understanding-pull-requests/",
    "title": "Understanding pull requests",
    "description": "An attempt to get my head around pull requests based on the\nfork and clone model as implemented in the `usethis` package.",
    "author": [
      {
        "name": "Brandon Farr",
        "url": {}
      }
    ],
    "date": "2021-02-10",
    "categories": [
      "usethis",
      "git"
    ],
    "contents": "\n\nContents\nFork and clone\nDevelop\nCreate pull request\nSummary\nNotes\n\n“Fork and clone” is the basis for issuing the “pull requests” that are the lifeblood of open source development on github in the R universe. The purpose of this What I Learned Today post is to understand this workflow from two levels:\nthe git/github concepts\nthe usethis helper functions\nGood reference information is found here.\nFork and clone\nTo begin a “fork and clone” within the “happygitwithr” / usethis recommended workflow, run:\n\n\nusethis::create_from_github(\"OWNER/repo\", \"local_dir\", fork = TRUE)\n\n\n\n\n\n\nThis accomplishes three things:\nA special copy, a fork, of THEM/REPO is made to your github account, referred to here as YOU/REPO.\nYOU/REPO is cloned to local_dir on your computer.\ngit “remotes” are setup as follows:\norigin: YOU/REPO, can push and pull\nupstream: THEM/REPO, can pull, can’t push\n\nBest “pull request” practices dictate working on a local branch in order to avoid local merge conflicts and to ease the merge process for OWNER once the “pull request” is issued. To establish a local branch on which to do your work, run:\n\n\nusethis::pr_init(\"dev_branch\")\n\n\n\nThis creates and switches to a local development branch, local/dev_branch, that is separate from the local/main branch. The allows your local repo to incorporate changes in upstream, i.e. keep up-to-date with development in OWNER/REPO while you develop locally.\nDevelop\nLocal development\nDevelop as usual, committing to local/dev_branch as appropriate via git tools included with Rstudio.\nKeeping up with OWNER\nAs necessary, and certainly recommended prior to issuing a “pull request”, pull down any changes happening on upstream/main by running:\n\n\nusethis::pr_merge_main()\n\n\n\n\n\n\nThis brings local/main up-to-date with all development on upstream/main. This should happen without any issue if you haven’t touched local/main, because that is where the updates are pulled to.\nMy current understanding is that this doesn’t perform any merge analysis between local/main and local/dev_branch.\nCreate pull request\nWhen you are ready to make the pull request, run the following:\n\n\nusethis::pr_push()\n\n\n\n\n\n\nThis accomplishes the following:\npushes your changes to your forked repo, YOU/REPO\nlaunches a browser window with the github page of OWNER/REPO\nmakes available the “pull request” option within github\nAt this point, you work with submitting the pull request on github. For the purposes of this post, assume the pull request is accepted by OWNER. You finish the pull request locally by running:\n\n\nusethis::pr_finish()\n\n\n\nThis once again brings local/main up-to-date with upstream/main, which now includes the changes incorporated in the accepted pull request. It also has the effect of switching back to the default branch, main and deleting the development branch dev_branch.\nTo make sure OWNER/REPO receives the changes of the merged pull request, in a shell run:\n\n$ git push -f origin main\n\nSummary\n\n\n# 1. fork and clone\nusethis::create_from_github(\"OWNER/repo\", \"local_dir\", fork = TRUE)\n\n# 2. create development branch\nusethis::pr_init(\"dev_branch\")\n\n# do all development work, committing locally on local/dev_branch\n\n# 3. pull changes from upstream\nusethis::pr_merge_main()\n\n# 4. initial pull request\nusethis::pr_push()\n\n# 5. clean up after pull request has been accepted and merged\nusethis::pr_finish()\n\n\n\nNotes\nCalled a “pull request”, because at the end of the day, YOU are asking OWNER to pull your changes into the OWNER/REPO/main.\n“formal” names, like OWNER/REPO and “aliases” like upstream have been used interchangeably (sloppily?) to reinforce the concept that “aliases” are useful shorthand for “formal” names\nMy sense is that the forked copy that sits in YOU is there largely for the purpose of being the basis of the pull request. In the documented usethis workflow, pr_push() is the only time that YOU/REPO gets updated during development of the pull request.\n\n\n\n",
    "preview": "posts/2021-02-10-understanding-pull-requests/images/fork-and-clone.png",
    "last_modified": "2021-02-10T19:26:35-05:00",
    "input_file": "understanding-pull-requests.utf8.md"
  },
  {
    "path": "posts/2021-01-30-better-stock-correlation-analysis/",
    "title": "Better Stock Correlation Analysis",
    "description": "Building on the previous post, with some considerations towards efficiency\nand increasing depth of analysis.",
    "author": [
      {
        "name": "Brandon Farr",
        "url": {}
      }
    ],
    "date": "2021-01-30",
    "categories": [
      "tidyquant",
      "tibbletime",
      "finance"
    ],
    "contents": "\nShort-comings of previous analysis\ntoo many steps to get correlations\ndon’t need to calculate all correlation pairs, just pairs including AAPL\npoint-in-time versus evolution of correlation\nneeds a plot, likely interactive (plotly)\n\n\nlibrary(tidyverse)  # for tidy/dplyr work\nlibrary(rvest)      # for web-scraping\nlibrary(tidyquant)  # for quant work (tiingo pricing, tq_mutate ...)\n\n\n\nAPI Consideration - repeated pulls\nGetting to a completed post is an iterative process, that involves: work, knitting the document, assessing the current state, deciding what to add, then repeating until the final version. This leads to repetitive pulls of the data from Tiingo. So, I begin by executing a single pull and storing it, so that we avoid this repetition.\nRun this chunk once while working on the post, then set chunk options to eval=FALSE and echo=TRUE so that the code displays, but is not executed.\n\n\n# create S&P 500 constituent table\nsp500_tbl <- read_html(\n  \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n) %>%\n  html_element(\"#constituents\") %>%\n  html_table() %>%\n  janitor::clean_names()\n\nwrite_csv(sp500_tbl, \"data/sp500.csv\")\n\n# create AAPL peer pricing table\npeer_pricing_tbl <- sp500_tbl %>%\n  filter(\n    gics_sub_industry == sp500_tbl %>% filter(symbol == \"AAPL\") %>% pull(gics_sub_industry)\n  ) %>%\n  pull(symbol) %>%\n  tq_get(.x, get = \"tiingo\")\n\nwrite_csv(peer_pricing_tbl, \"data/peer_pricing.csv\")\n\n\n\nOnce this chunk has been run successfully, just load the data from the local csvs:sp500.csvandpeer_pricing.csv`.\n\n\nsp500_tbl <- read_csv(\"data/sp500.csv\")\npeer_pricing_tbl <- read_csv(\"data/peer_pricing.csv\")\n\nglimpse(sp500_tbl)\n\n\nRows: 505\nColumns: 9\n$ symbol                <chr> \"MMM\", \"ABT\", \"ABBV\", \"ABMD\", \"ACN\", …\n$ security              <chr> \"3M Company\", \"Abbott Laboratories\", …\n$ sec_filings           <chr> \"reports\", \"reports\", \"reports\", \"rep…\n$ gics_sector           <chr> \"Industrials\", \"Health Care\", \"Health…\n$ gics_sub_industry     <chr> \"Industrial Conglomerates\", \"Health C…\n$ headquarters_location <chr> \"St. Paul, Minnesota\", \"North Chicago…\n$ date_first_added      <chr> \"1976-08-09\", \"1964-03-31\", \"2012-12-…\n$ cik                   <dbl> 66740, 1800, 1551152, 815094, 1467373…\n$ founded               <chr> \"1902\", \"1888\", \"2013 (1888)\", \"1981\"…\n\nglimpse(peer_pricing_tbl)\n\n\nRows: 1,764\nColumns: 14\n$ symbol      <chr> \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\",…\n$ date        <dttm> 2020-01-31, 2020-02-03, 2020-02-04, 2020-02-05…\n$ open        <dbl> 320.9, 304.3, 315.3, 323.5, 322.6, 322.4, 314.2…\n$ high        <dbl> 322.7, 313.5, 319.6, 324.8, 325.2, 323.4, 321.6…\n$ low         <dbl> 308.3, 302.2, 313.6, 318.9, 320.3, 318.0, 313.9…\n$ close       <dbl> 309.5, 308.7, 318.9, 321.4, 325.2, 320.0, 321.6…\n$ volume      <dbl> 49897096, 43496401, 34154134, 29706718, 2635638…\n$ adjusted    <dbl> 76.71, 76.50, 79.03, 79.67, 80.61, 79.51, 79.89…\n$ adjHigh     <dbl> 79.98, 77.70, 79.22, 80.49, 80.61, 80.35, 79.89…\n$ adjLow      <dbl> 76.41, 74.91, 77.74, 79.05, 79.38, 79.01, 77.98…\n$ adjOpen     <dbl> 79.54, 75.42, 78.15, 80.19, 79.95, 80.09, 78.06…\n$ adjVolume   <dbl> 199588384, 173985604, 136616536, 118826872, 105…\n$ divCash     <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.77, 0.00, 0.00,…\n$ splitFactor <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n\n# calculate returns as well\npeer_returns_tbl <- peer_pricing_tbl %>%\n  group_by(symbol) %>%\n  tq_transmute(adjusted, periodReturn, period = \"daily\", col_rename = \"return\")\n\n\n\nFewer steps, fewer pairs\nBuild a table with AAPL as symbol_1 and peer stocks as symbol_2, using the fact that left_join will create 1-row for every match of date. This is a succinct way of creating all return pairs.\n\n\naapl_peer_returns_tbl <-\n  peer_returns_tbl %>%\n    filter(symbol == \"AAPL\") %>%\n    select(symbol, date, return) %>%\n  left_join(\n    peer_returns_tbl %>%\n      filter(symbol != \"AAPL\") %>%\n      select(symbol, date, return),\n    by = \"date\",\n    suffix = c(\"_1\", \"_2\")\n  ) %>%\n  select(date, matches(\"symbol\"), everything()) %>%\n  arrange(symbol_2, date) %>%\n  filter(!(return_1 == 0 & return_2 == 0))  # remove days where returns == 0\n\nglimpse(aapl_peer_returns_tbl)\n\n\nRows: 1,506\nColumns: 5\n$ date     <dttm> 2020-02-03, 2020-02-04, 2020-02-05, 2020-02-06, 2…\n$ symbol_1 <chr> \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\", \"A…\n$ symbol_2 <chr> \"HPE\", \"HPE\", \"HPE\", \"HPE\", \"HPE\", \"HPE\", \"HPE\", \"…\n$ return_1 <dbl> -0.0027463, 0.0330137, 0.0081543, 0.0116970, -0.01…\n$ return_2 <dbl> 0.015075, 0.019802, 0.032594, 0.002015, -0.012064,…\n\nNow, calculating correlations is simple via a single summarize call. Note: the use parameter of cor is left as the default “everything” which will cause NA to appear if there are any data issues. This is preferable to just returning a result, as we don’t have any data robustness built into the data pipeline.\n\n\naapl_peer_returns_tbl %>%\n  group_by(symbol_2) %>%\n  summarize(\n    cor = cor(return_1, return_2)\n  )\n\n\n# A tibble: 6 x 2\n  symbol_2   cor\n* <chr>    <dbl>\n1 HPE      0.477\n2 HPQ      0.498\n3 NTAP     0.537\n4 STX      0.548\n5 WDC      0.472\n6 XRX      0.460\n\nIt can be seen that we are getting the same results with less effort, e.g. no pivoting when comparing the above to the results shown below.\n\n\npeer_returns_tbl %>% \n  \n  # put returns into columns\n  pivot_wider(names_from = symbol, values_from = return) %>%\n  \n  # first day return is 0, so remove row\n  slice(-1) %>%\n  \n  # remove date column, so you can call `cor` on entire tibble\n  select(-date) %>%\n  \n  # calculate correlations\n  cor() %>%\n  \n  # `cor` returns a matrix, convert back into tibble\n  as_tibble(rownames = \"symbol_1\") %>%\n  \n  # transform into tidy format for easier processing\n  pivot_longer(-1, names_to = \"symbol_2\", values_to = \"cor\") %>%\n  \n  # remove the correlations of a stock with itself\n  filter(!(symbol_1 == symbol_2)) %>%\n  \n  # group by symbol_1 and arrange descending by correlation\n  group_by(symbol_1) %>%\n  arrange(desc(cor)) %>%\n  \n  # look at first row in each group == highest correlated stock\n  slice(1)\n\n\n# A tibble: 7 x 3\n# Groups:   symbol_1 [7]\n  symbol_1 symbol_2   cor\n  <chr>    <chr>    <dbl>\n1 AAPL     STX      0.548\n2 HPE      NTAP     0.712\n3 HPQ      XRX      0.781\n4 NTAP     HPE      0.712\n5 STX      WDC      0.705\n6 WDC      STX      0.705\n7 XRX      HPQ      0.781\n\nCorrelation evolution\nThe above analysis produces a single estimate of correlation using all of the data in the dataset. Since the purpose of calculating these correlations is to find a substitute security for a short time frame, 30 days, it is probably better to have a sense of how stable the relationship between the two stocks is through time.\nFor this, we will use the rollify function from the tibbletime package. Using the examples given in the documentation makes this relatively simple\n\n\nlibrary(tibbletime)\n\ncor_roll <- rollify(~cor(.x, .y), window = 30)\n\naapl_cor_tbl <-\n  aapl_peer_returns_tbl %>%\n  group_by(symbol_2) %>%\n  mutate(cor = cor_roll(return_1, return_2))\n\n\n\ntibbletime really is a great convenience.\nNotice how the correlations are noisy (expected), but have in general declined significantly (unexpected).\n\n\naapl_cor_tbl %>%\n  ggplot(aes(x = date, y = cor, color = symbol_2)) +\n  geom_hline(yintercept = 0, color = \"gray40\") +\n  geom_line() +\n  ylim(-1, 1) +\n  labs(\n    title = \"AAPL Stock Correlation\",\n    subtitle = \"with GICS sub-industry peers\",\n    caption = \"30-trading day rolling windows\",\n    x = NULL, y = NULL, color = NULL\n  ) +\n  theme_tq() +\n  scale_color_tq() +\n  # for legend to have 1 row\n  guides(col = guide_legend(nrow = 1))\n\n\n\n\nA smoothed version makes this trend a little clearer.\n\n\ncode\n\naapl_cor_tbl %>%\n  ggplot(aes(x = date, y = cor, color = symbol_2)) +\n  geom_hline(yintercept = 0, color = \"gray40\") +\n  geom_smooth() + # default loess smoothing\n  \n  # gam smoothing\n  # geom_smooth(method = \"gam\", formula = y ~ s(x, bs = \"cs\")) +\n  ylim(-1, 1) +\n  labs(\n    title = \"AAPL Stock Correlation\",\n    subtitle = \"with GICS sub-industry peers\",\n    caption = \"30-trading day rolling windows\",\n    x = NULL, y = NULL, color = NULL\n  ) +\n  theme_tq() +\n  scale_color_tq() +\n  # for legend to have 1 row\n  guides(col = guide_legend(nrow = 1))\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-01-30-better-stock-correlation-analysis/better-stock-correlation-analysis_files/figure-html5/unnamed-chunk-7-1.png",
    "last_modified": "2021-01-30T19:18:07-05:00",
    "input_file": "better-stock-correlation-analysis.utf8.md"
  },
  {
    "path": "posts/2021-01-28-harvesting-s5-500-sectors/",
    "title": "Ha`rvest`ing S&P 500 Sectors",
    "description": "Using open source data to look at within industry correlations.",
    "author": [
      {
        "name": "Brandon Farr",
        "url": {}
      }
    ],
    "date": "2021-01-28",
    "categories": [
      "tidyquant",
      "rvest",
      "janitor",
      "finance"
    ],
    "contents": "\n\n\nlibrary(tidyverse)  # for tidy/dplyr work\nlibrary(rvest)      # for web-scraping\n\n\n\nA little work with the rvest package. Try to get the current S&P 500 constituents from Wikipedia.\n\n\nwiki_sp500 <- read_html(\n  \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n)\n\n\n\nAccording to my SelectorGadget work, the element I am looking for is “#constituents”\n\n\nwiki_sp500 %>%\n  html_element(\"#constituents\")\n\n\n{html_node}\n<table class=\"wikitable sortable\" id=\"constituents\">\n[1] <tbody>\\n<tr>\\n<th>\\n<a href=\"/wiki/Ticker_symbol\" title=\"Ticke ...\n\nSince this is a table, it needs to be passed through the html_table function.\n\n\nsp500_tbl <- wiki_sp500 %>%\n  html_element(\"#constituents\") %>%\n  html_table() %>%\n  janitor::clean_names()\n\nsp500_tbl\n\n\n# A tibble: 505 x 9\n   symbol security sec_filings gics_sector gics_sub_indust…\n   <chr>  <chr>    <chr>       <chr>       <chr>           \n 1 MMM    3M Comp… reports     Industrials Industrial Cong…\n 2 ABT    Abbott … reports     Health Care Health Care Equ…\n 3 ABBV   AbbVie … reports     Health Care Pharmaceuticals \n 4 ABMD   ABIOMED… reports     Health Care Health Care Equ…\n 5 ACN    Accentu… reports     Informatio… IT Consulting &…\n 6 ATVI   Activis… reports     Communicat… Interactive Hom…\n 7 ADBE   Adobe I… reports     Informatio… Application Sof…\n 8 AMD    Advance… reports     Informatio… Semiconductors  \n 9 AAP    Advance… reports     Consumer D… Automotive Reta…\n10 AES    AES Corp reports     Utilities   Independent Pow…\n# … with 495 more rows, and 4 more variables:\n#   headquarters_location <chr>, date_first_added <chr>, cik <int>,\n#   founded <chr>\n\nWow! Sure enough, we get a tibble with what we want. Notice the use of janitor to clean up the column names for convenience. I think I should always do this.\nLet’s look at all of the stocks in the same gics_sub_industry as Apple.\n\n\naapl_sub_ind <- sp500_tbl %>%\n  filter(symbol == \"AAPL\") %>%\n  pull(gics_sub_industry)\n\naapl_peers_tbl <- sp500_tbl %>%\n  filter(gics_sub_industry == aapl_sub_ind) %>%\n  select(symbol, security, matches(\"^gics\"))\n\naapl_peers_tbl\n\n\n# A tibble: 7 x 4\n  symbol security         gics_sector      gics_sub_industry          \n  <chr>  <chr>            <chr>            <chr>                      \n1 AAPL   Apple Inc.       Information Tec… Technology Hardware, Stora…\n2 HPE    Hewlett Packard… Information Tec… Technology Hardware, Stora…\n3 HPQ    HP Inc.          Information Tec… Technology Hardware, Stora…\n4 NTAP   NetApp           Information Tec… Technology Hardware, Stora…\n5 STX    Seagate Technol… Information Tec… Technology Hardware, Stora…\n6 WDC    Western Digital  Information Tec… Technology Hardware, Stora…\n7 XRX    Xerox            Information Tec… Technology Hardware, Stora…\n\nThis is a great list to pass to tidyquant and try pulling some pricing data.\n\n\nlibrary(tidyquant)\n\naapl_peers_pricing <- aapl_peers_tbl %>%\n  pull(symbol) %>%\n  tq_get(get = \"tiingo\")\n\n\n\nBy default, we get about a years worth of prices. Let’s calculate returns:\n\n\naapl_peers_returns <- aapl_peers_pricing %>%\n  group_by(symbol) %>%\n  tq_transmute(adjusted, periodReturn, period = \"daily\", col_rename = \"return\")\n\n\n\nNow, calculate the correlation matrix:\n\n\naapl_peers_returns %>%\n  \n  # put returns into columns\n  pivot_wider(names_from = symbol, values_from = return) %>%\n  \n  # first day return is 0, so remove row\n  slice(-1) %>%\n  \n  # remove date column, so you can call `cor` on entire tibble\n  select(-date) %>%\n  \n  # calculate correlations\n  cor() %>%\n  \n  # `cor` returns a matrix, convert back into tibble\n  as_tibble(rownames = \"symbol_1\") %>%\n  \n  # transform into tidy format for easier processing\n  pivot_longer(-1, names_to = \"symbol_2\", values_to = \"cor\") %>%\n  \n  # remove the correlations of a stock with itself\n  filter(!(symbol_1 == symbol_2)) %>%\n  \n  # group by symbol_1 and arrange descending by correlation\n  group_by(symbol_1) %>%\n  arrange(desc(cor)) %>%\n  \n  # look at first row in each group == highest correlated stock\n  slice(1)\n\n\n# A tibble: 7 x 3\n# Groups:   symbol_1 [7]\n  symbol_1 symbol_2   cor\n  <chr>    <chr>    <dbl>\n1 AAPL     STX      0.550\n2 HPE      NTAP     0.713\n3 HPQ      XRX      0.781\n4 NTAP     HPE      0.713\n5 STX      WDC      0.711\n6 WDC      STX      0.711\n7 XRX      HPQ      0.781\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-01-28T18:38:58-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-26-distill-and-timetk/",
    "title": "distill and timetk",
    "description": "today's directed wanderings",
    "author": [
      {
        "name": "Brandon Farr",
        "url": {}
      }
    ],
    "date": "2021-01-26",
    "categories": [
      "timetk",
      "distill"
    ],
    "contents": "\nToday, I worked on the following:\nsetting up this blog\nrunning through the weekly Free R Tip from business-science\nread through GIT SQL Together\nDistill blog\nSet up this blog using the distill package and github pages. A few takeaways:\nsee here for details on getting your repo right\nsee About for other details\nFree R Tip\nThis week’s tip was about many ways to create data frames/tibbles. I was familiar with these, but found the use of timetk::tk_make_timeseries informative.\n\n\ntibble::tibble(\n    date          = timetk::tk_make_timeseries(\"2010\", length_out = 12, by = \"quarter\"),\n    interest_rate = (seq(12, 3, length.out = 12) * (sin(1:12) + 2)) / 12\n)\n\n\n# A tibble: 12 x 2\n   date       interest_rate\n   <date>             <dbl>\n 1 2010-01-01         2.84 \n 2 2010-04-01         2.71 \n 3 2010-07-01         1.85 \n 4 2010-10-01         0.989\n 5 2011-01-01         0.757\n 6 2011-04-01         1.13 \n 7 2011-07-01         1.57 \n 8 2011-10-01         1.56 \n 9 2012-01-01         1.10 \n10 2012-04-01         0.563\n11 2012-07-01         0.318\n12 2012-10-01         0.366\n\nNote that the by argument is very flexible:\nby - a character string, containing one of “sec”, “min”, “hour”, “day”, “week”, “month”, “quarter” or “year”. You can create regularly spaced sequences using phrases like by = “10 min”\nGIT + SQL\nYou will always need that query again, so “GIT” it\nQueries change through time, you may need a previous version when you realize the improved query “broke” something in a way that wasn’t immediately obvious\nsave with a .sql extension\n\nLooks like there are other interesting posts from this site. I added one to Notion, so it may be covered here someday.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-01-26T16:28:26-05:00",
    "input_file": {}
  }
]
